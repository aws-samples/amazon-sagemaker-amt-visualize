{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70352b29-fc66-4d69-8d4f-d3f4b4019f6e",
   "metadata": {},
   "source": [
    "# Advanced Tuning With Custom Training and Visualization\n",
    "\n",
    "In our getting started notebook, [1_tuning_of_builtin_xgboost.ipynb](https://github.com/aws-samples/amazon-sagemaker-amt-visualize/blob/main/1_tuning_of_builtin_xgboost.ipynb) we dipped our feet into water and trained a single hyperparameter optimization (HPO) job with Amazon SageMaker Automatic Model Tuning (AMT). We trained and tuned a built-in XGBoost model provided by SageMaker Algorithms, meaning we did not need to provide our own training script. In the tuning stage, we ran a single HPO job with the bayesian search method as our HPO strategy.\n",
    "\n",
    "But there is still much to learn about our hyperparamter space. AMT provides us with several strategies to sample values from this space, namely Grid, Random, Beyesian search and the Hyperband strategy. To illustrate the effect these strategies have on our HPO trials, we will run two identical HPO jobs, one using Bayesian and the other with the Random search strategy and we will compare the results side-by-side. \n",
    "\n",
    "Using the information we learned about our hyperparameter space from our first job, we will modify our parameter tuning ranges to explore more optimal regions of the space. With these newly defined parameter ranges, we will run a second HPO job. However we do not want our tuner to start completely from scratch. For more optimal and guided performance, we want to somehow reuse the information learned from our first HPO job, as a start point in our second. To do achieve this, we will use SageMaker AMT's warm start feature, by passing our previous HPO jobs as parents to our new HPO job. Our new HPO job will begin with an awareness of what values in our hyperparameter space were already explored by its parents, and which regions of this space were most promising. This knowledge will influence AMT's decision when selecting the hyperparameters for our HPO job and allow AMT to start with more optimital values earlier. We will see how trails ran with warm starting generally start with highly optimal values, when compared to trials without warm starts.\n",
    "\n",
    "Additionally, we will take a step towards customizing our training, by replacing the SageMaker built-in algorithm with our own custom training script.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook is split into the following sections:\n",
    "- Setup and Imports\n",
    "- Load and Prepare dataset\n",
    "- Train a Random Forest Classifier using a custom script with SageMaker Training\n",
    "- Tune Hyperparameters with SageMaker AMT using Random & Bayesian strategies\n",
    "- Review Results and Refine the search space\n",
    "- Warmstarting\n",
    "\n",
    "## Overview SageMaker **Training**\n",
    "\n",
    "![Overview SageMaker Training](img/training_script_mode.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c090cbfa",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32b8dee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f00dec-00da-4131-935d-e48f52fb5cd0",
   "metadata": {},
   "source": [
    "We ran this notebook using Amazon SageMaker with the version you see in the output of the next cell below. If your version is lower and you encounter issues, we recommend uncommenting the code below to upgrade your pip and SageMaker versions. Make sure to restart your kernel after upgrading for the changes to take effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96d5ff6d-f241-41e7-a424-f22a697b5991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.116.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker.__version__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0066da5-e1e5-4c8c-a0d3-d5b1fcced0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade --quiet pip \"sagemaker>=2.116.0\" # upgrade SageMaker to the recommended version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "857237db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4669f27d-6868-4e8f-80b4-52a084c1f339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDK setup\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "sm = boto3.client('sagemaker')\n",
    "boto_sess = boto3.Session(region_name=region)\n",
    "sm_sess = sagemaker.session.Session(boto_session=boto_sess, sagemaker_client=sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0c960ff-3d16-40cd-9285-526bb7e0a413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data layout and locations. \n",
    "# To store our data we are using a prefix in the Amazon SageMaker default bucket. Feel free to adjust to your preferences.\n",
    "\n",
    "BUCKET = sm_sess.default_bucket()\n",
    "PREFIX = 'amt-visualize-demo'\n",
    "s3_data_url = f's3://{BUCKET}/{PREFIX}/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ba3311-d737-42c6-ba4b-9bc7cb2df5fa",
   "metadata": {},
   "source": [
    "## Load and Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de1cc644-4914-4429-a143-e33a8c438b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a9d92e-a6a0-461d-84b9-ddc45c1ac238",
   "metadata": {},
   "source": [
    "The focus of this notebook is on Hyperparameter Optimization. Hence the actual task and data only play a supporting role. But to give some brief context, we are optimizing the hyperparameters of a Random Forest model that should classify handwritten digits.\n",
    "\n",
    "We use the Optical Recognition of Handwritten Digits Data Set via scikit-learn:\n",
    "\n",
    "_Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51e6e391",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "digits         = datasets.load_digits()\n",
    "digits_df      = pd.DataFrame(digits.data)\n",
    "digits_df['y'] = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf5b3d01-4282-4249-a7cd-46a0e52e23b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digit: 4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "print('digit:', int(digits_df.iloc[100].y))\n",
    "plt.imshow(digits_df.iloc[100, 1:].values.reshape(8, -1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edddc672-ba8b-4827-b610-4077ab386e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits_df.to_csv('data/digits.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40337860-8dd5-400e-acda-08c3472d04b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: data/digits.csv to s3://sagemaker-us-east-1-632581975302/amt-visualize-demo/data/digits.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 sync data/ {s3_data_url} --exclude '*' --include 'digits.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350aee6a",
   "metadata": {},
   "source": [
    "## Preparing Training Scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d2950f-b81f-444d-a343-440921f4b09c",
   "metadata": {},
   "source": [
    "Below we define our training script that we will pass on to SageMaker to run our training. Note that we chose to directly write the python file from the our Jupyter Notebook to the `src/` directory using the command `%%writefile src/train.py`. This is to make this sample more readable and keep all relevant code in one place in this notebook. We place our generated files in a /src directory locally in the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12de36fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p src\n",
    "import sys\n",
    "sys.path.append('src')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b750db7-2f71-44b5-bcd5-60182220b70e",
   "metadata": {},
   "source": [
    "### Dependencies and training containers \n",
    "When running our training, we often rely on some functions from other libraries and dependecies. In our case, we use functions from the `scikit-learn` and `pandas` libraries.\n",
    "\n",
    "Luckily, SageMaker offers [several Frameworks](https://sagemaker.readthedocs.io/en/stable/frameworks/index.html) that come pre-installed with popular data science and ML frameworks such Scikit-learn, PyTorch and TensorFlow.\n",
    "\n",
    "For our purposes in this notebook, we'll be using the [Scikit-learn Framework](https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/using_sklearn.html#prepare-a-scikit-learn-training-script) that has everything we need for our training.\n",
    "\n",
    "### (Optional) Defining a requirements.txt file\n",
    "One common practice to collect and organise dependencies and external libraries in Python, is to define a `requirements.txt` file containing a list of imported libraries and their versions.\n",
    "\n",
    "Although our Scikit-learn container provides us with all the libraries we need, if we needed access to libraries not included in the provided container, we could also define these in `requirements.txt`.\n",
    "\n",
    "Defining a `requirements.txt` file is an optional step, and not required to launch SageMaker training but it is supported if you wish to include it. SageMaker will automatically scan our source directory containing our `train.py` for an additional `requirements.txt`. For more information, see the [documentation here](https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/using_sklearn.html#using-third-party-libraries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ddb5499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/requirements.txt\n",
    "\n",
    "# Not necessary for our training but we may define additional libraries here as required\n",
    "#[optional-additional-libraries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30126cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/train.py\n",
    "\n",
    "#!/usr/bin/env python\n",
    "import argparse\n",
    "import os\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import joblib\n",
    "\n",
    "def fit(train_dir, n_estimators, max_depth, min_samples_leaf, max_features, min_weight_fraction_leaf):\n",
    "    \n",
    "    digits = pd.read_csv(Path(train_dir)/'digits.csv')\n",
    "    \n",
    "    Xtrain, Xtest, ytrain, ytest = train_test_split(digits.iloc[:, :-1], digits.iloc[:, -1], test_size=.2)\n",
    "    \n",
    "    m = RandomForestClassifier(n_estimators=n_estimators, \n",
    "                               max_depth=max_depth, \n",
    "                               min_samples_leaf=min_samples_leaf,\n",
    "                               max_features=max_features,\n",
    "                               min_weight_fraction_leaf=min_weight_fraction_leaf)\n",
    "    m.fit(Xtrain, ytrain)\n",
    "    predicted = m.predict(Xtest)\n",
    "    pre, rec, f1, _ = precision_recall_fscore_support(ytest, predicted, pos_label=1, average='weighted')\n",
    "    \n",
    "    print(f'pre: {pre:5.3f} rec: {rec:5.3f} f1: {f1:5.3}')\n",
    "    \n",
    "    return m\n",
    "\n",
    "if __name__ =='__main__':\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script.\n",
    "\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "\n",
    "    parser.add_argument('--n-estimators', type=int, default=100)\n",
    "    parser.add_argument('--max-depth', type=int, default=10)\n",
    "    parser.add_argument('--min-samples-leaf', type=int, default=1)\n",
    "    parser.add_argument('--max-features', type=str, default='auto')\n",
    "    parser.add_argument('--min-weight-fraction-leaf', type=float, default=0.01)\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "    trained_model = fit(train_dir=args.train, \n",
    "                        n_estimators=args.n_estimators, \n",
    "                        max_depth=args.max_depth,\n",
    "                        min_samples_leaf=args.min_samples_leaf,\n",
    "                        max_features=args.max_features,\n",
    "                        min_weight_fraction_leaf=args.min_weight_fraction_leaf)\n",
    "    \n",
    "    joblib.dump(trained_model, os.path.join(args.model_dir, 'model.joblib'))   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b54c2a5-74c1-4ef6-8138-8c1c8023cbe7",
   "metadata": {},
   "source": [
    "#### Test run our training script locally\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1334ead3-3a0f-4d2a-8e0e-899198f53fb6",
   "metadata": {},
   "source": [
    "Now that we have prepared our training script, let's quickly run it locally to check that it works. We call and run our training script directly from this notebook, which makes it possible to use Python Debugger and other utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "267c9161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre: 0.963 rec: 0.961 f1: 0.961\n",
      "CPU times: user 392 ms, sys: 7.5 ms, total: 400 ms\n",
      "Wall time: 464 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=10, min_weight_fraction_leaf=0.01)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Running the code from within the notebook. It would then be possible to use the Python Debugger, pdb.\n",
    "from train import fit\n",
    "fit('data', 100, 10, 1, 'auto', 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b58493-ba2e-42e0-843b-54f62dee0f76",
   "metadata": {},
   "source": [
    "We could also run it directly from the command line. Doing so would trigger the `if __name__ =='__main__':` statement in our script, to parse our command line arguments. When SageMaker loads the container with our script into the training instance, SageMaker will also launch our train.py via the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a77f5686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre: 0.948 rec: 0.947 f1: 0.947\n"
     ]
    }
   ],
   "source": [
    "!cd src && python train.py --train ../data/ --model-dir /tmp/ --n-estimators 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9745988b",
   "metadata": {},
   "source": [
    "## Run Training in Containers\n",
    "\n",
    "With our `train.py` ready and tested, we are ready to create our estimator and call `fit()` to start our training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a34781d-a942-4496-a274-ae285f71a77c",
   "metadata": {},
   "source": [
    "### Define our metrics\n",
    "\n",
    "Recall that in our first notebook, we used a built-in Algorithm for our training. This pre-built container and training script by SageMaker came already configured with its own metrics and metrics defintions. Since we are using a custom training script in this notebook, we now must specify our metrics to SageMaker.\n",
    "\n",
    "In our `train.py`, we used `print(f'pre: {pre:5.3f} rec: {rec:5.3f} f1: {f1:5.3}')` to emit metrics from our training. During training, this information is printed to `std.out`, which is parsed by SageMaker and sent to Amazon CloudWatch.\n",
    "\n",
    "In order for SageMaker to successfully parse our metrics, we must provide the Regular Expressions (REGEX), that instruct SageMaker how to extract the numerical values from our print statement above, as described [in the documentation here](https://docs.aws.amazon.com/sagemaker/latest/dg/training-metrics.html). We do this with our `metric_definitions` object below, which we then pass as a parameter to our estimator. \n",
    "\n",
    "_Note: The metrics we chose here will eventually be used by SageMaker AMT for our HPO jobs, more [information on tuning metrics can be found here](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-define-metrics.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "431c83f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions = [{'Name': 'Valid:Precision',  'Regex': r'pre:\\s+(-?[0-9\\.]+)'},\n",
    "                      {'Name': 'Valid:Recall',     'Regex': r'rec:\\s+(-?[0-9\\.]+)'},\n",
    "                      {'Name': 'Valid:F1',         'Regex': r'f1:\\s+(-?[0-9\\.]+)'}]                                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adab126f-b09a-4161-9257-b57d942eacc2",
   "metadata": {},
   "source": [
    "In the following cells, we define our estimator, in which we pass our training script, indicate our source directory and specify our training instance configuration with the `instance_type` and `instance_count` variables.\n",
    "\n",
    "Notice that, we are using an SKLearn estimator, which instructs SageMaker to load our training script onto a container that comes with Scikit-learn and other dependencies pre-installed. The `framework_version` parameter in the estimator defines which Scikit-learn version we wish to run. You  can read more about the parameters [here](https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/sagemaker.sklearn.html)\n",
    "\n",
    "This differs from the estimator used in our [1_tuning_of_builtin_xgboost.ipynb](https://github.com/aws-samples/amazon-sagemaker-amt-visualize/blob/main/1_tuning_of_builtin_xgboost.ipynb) notebook, which we used to run built-in algorithms rather than custom training scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "670321e1-7268-4432-a7f8-973fd4aa8de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2cf57ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = SKLearn(\n",
    "    'train.py',\n",
    "    source_dir='src',\n",
    "    role=get_execution_role(),\n",
    "    instance_type= 'ml.m5.large',\n",
    "    instance_count=1,\n",
    "    framework_version='0.23-1',\n",
    "    metric_definitions=metric_definitions,\n",
    "\n",
    "    use_spot_instances= True,\n",
    "    max_run=  60 * 60 * 24,\n",
    "    max_wait= 60 * 60 * 24,\n",
    "\n",
    "    hyperparameters = {'n-estimators': 100,\n",
    "                       'max-depth': 10,\n",
    "                       'min-samples-leaf': 1,\n",
    "                       'max-features': 'auto',\n",
    "                       'min-weight-fraction-leaf': 0.1}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7cf9d874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-632581975302/amt-visualize-demo/data\n",
      "2022-11-21 15:35:56 Starting - Starting the training job...\n",
      "2022-11-21 15:36:19 Starting - Preparing the instances for trainingProfilerReport-1669044956: InProgress\n",
      "............\n",
      "2022-11-21 15:38:20 Downloading - Downloading input data...\n",
      "2022-11-21 15:38:49 Training - Downloading the training image...\n",
      "2022-11-21 15:39:25 Uploading - Uploading generated training model\u001b[34m2022-11-21 15:39:16,681 sagemaker-containers INFO     Imported framework sagemaker_sklearn_container.training\u001b[0m\n",
      "\u001b[34m2022-11-21 15:39:16,685 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-11-21 15:39:16,694 sagemaker_sklearn_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-11-21 15:39:17,045 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.2.2 -> 22.3.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2022-11-21 15:39:18,325 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-11-21 15:39:18,336 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-11-21 15:39:18,346 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-11-21 15:39:18,355 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_sklearn_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"max-depth\": 10,\n",
      "        \"max-features\": \"auto\",\n",
      "        \"min-samples-leaf\": 1,\n",
      "        \"min-weight-fraction-leaf\": 0.1,\n",
      "        \"n-estimators\": 100\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-scikit-learn-2022-11-21-15-35-56-000\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-632581975302/sagemaker-scikit-learn-2022-11-21-15-35-56-000/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 2,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.large\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.large\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"max-depth\":10,\"max-features\":\"auto\",\"min-samples-leaf\":1,\"min-weight-fraction-leaf\":0.1,\"n-estimators\":100}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.large\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=2\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-632581975302/sagemaker-scikit-learn-2022-11-21-15-35-56-000/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_sklearn_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"max-depth\":10,\"max-features\":\"auto\",\"min-samples-leaf\":1,\"min-weight-fraction-leaf\":0.1,\"n-estimators\":100},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-scikit-learn-2022-11-21-15-35-56-000\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-632581975302/sagemaker-scikit-learn-2022-11-21-15-35-56-000/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":2,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.large\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--max-depth\",\"10\",\"--max-features\",\"auto\",\"--min-samples-leaf\",\"1\",\"--min-weight-fraction-leaf\",\"0.1\",\"--n-estimators\",\"100\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_MAX-DEPTH=10\u001b[0m\n",
      "\u001b[34mSM_HP_MAX-FEATURES=auto\u001b[0m\n",
      "\u001b[34mSM_HP_MIN-SAMPLES-LEAF=1\u001b[0m\n",
      "\u001b[34mSM_HP_MIN-WEIGHT-FRACTION-LEAF=0.1\u001b[0m\n",
      "\u001b[34mSM_HP_N-ESTIMATORS=100\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/miniconda3/bin:/miniconda3/lib/python37.zip:/miniconda3/lib/python3.7:/miniconda3/lib/python3.7/lib-dynload:/miniconda3/lib/python3.7/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python train.py --max-depth 10 --max-features auto --min-samples-leaf 1 --min-weight-fraction-leaf 0.1 --n-estimators 100\u001b[0m\n",
      "\u001b[34mpre: 0.880 rec: 0.869 f1: 0.868\u001b[0m\n",
      "\u001b[34m2022-11-21 15:39:19,835 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-11-21 15:40:00 Completed - Training job completed\n",
      "Training seconds: 97\n",
      "Billable seconds: 39\n",
      "Managed Spot Training savings: 59.8%\n"
     ]
    }
   ],
   "source": [
    "print(s3_data_url)\n",
    "estimator.fit({'train': s3_data_url}, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c47cd0-bead-449c-8bd5-e4a7627aca84",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Overview SageMaker Automatic Model Tuning\n",
    "\n",
    "Now we can explore more possible `hyperparameters` for the same `estimator` as we have used above. As shown on the diagram, we setup the `HyperparameterTuner` using our custom training script and launch HPO Job. This will initiate Training Jobs wth different hyperparameters from `hpt_ranges`. \n",
    "\n",
    "![Overview SageMaker Automatic Model Tuning](img/amt_script_mode.png)\n",
    "\n",
    "### What is necessary to define for [`HyperparameterTuner`](https://sagemaker.readthedocs.io/en/stable/api/training/tuner.html)?\n",
    "\n",
    "1. the ranges of hyperparameters\n",
    "\n",
    "The hyperparameters are different from algorithm to algorithm. In our example we use scikit-learn implementation of Random Forest, so you can look into all its hyperparameters [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).\n",
    "The ranges are defined with the types of hyperparameter. There are three of them such as \n",
    "- categorical parameters that are defined in a concrete set\n",
    "- continuous parameters with any real number values between the pre-defined minimal and maximal values\n",
    "- integer parameters with any integer value between the minimum and maximum values. \n",
    "Additional information on hyperparameters structure and the step between them (scaling) can be found [here](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-define-ranges.html).\n",
    "\n",
    "2. the objective metric for the Tuning Job to optimize \n",
    "\n",
    "The metric is necessary for custom training code, a built-in SageMaker algorithm can use default values. You can check out [Define Metrics documentation page](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-define-ranges.html) to see the examples of metric definitions and HPO logs.  \n",
    "\n",
    "\n",
    "3. the optimization strategy\n",
    "\n",
    "How will the HPO job choose concrete hyperparameters? There are four strategies currently available that you can use for SageMaker AMT:\n",
    "- Grid Search\n",
    "- Random Search \n",
    "- Bayesian Optimization (default)\n",
    "- Hyperband.\n",
    "\n",
    "Let's look into their implementation and differences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad60914-ad1e-4658-85e1-bbd6ffb15663",
   "metadata": {},
   "source": [
    "## Automatic Model Tuning Jobs - Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a88fcf-116c-42e3-8d0b-a3ebf2e77506",
   "metadata": {},
   "source": [
    "With Random Search we can define `hpt_ranges` in continuous manner and AMT will choose the random combinations. This strategy normally has lower complexity of computation compared to a strategy such as Grid Search and it can be controlled through defining the maximum number of trials (`max_jobs`) and the number of workers for parallelization (`max_parallel_jobs`). \n",
    "\n",
    "For the purposes of our visualizations later on in the notebook, we stick to a value of `k=2` for `max_parallel_jobs`, across all strategies to create a visually compelling narrative in our visualizations. This will become clear further on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "36cc986b-8fea-48ea-ae89-cdcce1106ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter\n",
    "from sagemaker.tuner import ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "n = 30\n",
    "k = 2\n",
    "\n",
    "hpt_ranges = {'n-estimators': IntegerParameter(1, 200),\n",
    "              'max-depth': IntegerParameter(1, 20), \n",
    "              'min-samples-leaf': IntegerParameter(1, 10),\n",
    "              'min-weight-fraction-leaf': ContinuousParameter(0.01, 0.5),\n",
    "              'max-features': CategoricalParameter(['auto', 'log2', 'sqrt'])}\n",
    "\n",
    "tuner_parameters = {'estimator': estimator,\n",
    "                    'base_tuning_job_name': 'random',\n",
    "                    'metric_definitions': metric_definitions,\n",
    "                    \n",
    "                    'objective_metric_name': 'Valid:F1',\n",
    "                    'objective_type': 'Maximize',\n",
    "                    'hyperparameter_ranges': hpt_ranges,\n",
    "                    'strategy': 'Random',\n",
    "                    \n",
    "                    'max_jobs': n,\n",
    "                    'max_parallel_jobs': k} # With 'random' we could use a much higher parallelization here, \n",
    "                                            # as all trials/jobs are independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "13243577-d95b-4602-b9d1-76c5e2415fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuning job submitted: random-221121-1556.\n"
     ]
    }
   ],
   "source": [
    "random_tuner = HyperparameterTuner(**tuner_parameters)\n",
    "random_tuner.fit({'train': s3_data_url}, wait=False)\n",
    "random_tuner_name = random_tuner.describe()['HyperParameterTuningJobName']\n",
    "print(f'tuning job submitted: {random_tuner_name}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f4ed63-8dbe-41ff-a8f9-0555e3bb000e",
   "metadata": {},
   "source": [
    "## Automatic Model Tuning Jobs - Bayesian Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d129d3-b552-42ca-ac7a-c918d3685fdd",
   "metadata": {},
   "source": [
    "Bayesian Optimization treats optimization as a regression problem and takes into account the past evaluation outcomes of each run to sample new candidates that are more likely to optimize the objective metric, thus, trying to arrive at optimised set of hyperparameters as soon as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "04abac5e-921d-4cfc-be69-46ea1fc1626b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_parameters['strategy']             = 'Bayesian'\n",
    "tuner_parameters['base_tuning_job_name'] = 'bayesian'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "594efe00-2337-4361-88ad-3d310a8e85bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuning job submitted: bayesian-221121-1556.\n"
     ]
    }
   ],
   "source": [
    "bayesian_tuner = HyperparameterTuner(**tuner_parameters)\n",
    "bayesian_tuner.fit({'train': s3_data_url}, wait=False)\n",
    "bayesian_tuner_name = bayesian_tuner.describe()['HyperParameterTuningJobName']\n",
    "print(f'tuning job submitted: {bayesian_tuner_name}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff256cb1-2591-4d6d-bc56-2b343537c353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................................................................"
     ]
    }
   ],
   "source": [
    "random_tuner.wait()\n",
    "bayesian_tuner.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb570a14-249b-4061-b07e-580c429cc8ba",
   "metadata": {},
   "source": [
    "## Review Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a277dc9-d991-4ece-ac9f-08b721ac1669",
   "metadata": {},
   "outputs": [],
   "source": [
    "from amtviz import visualize_tuning_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab7470c-bd1f-4bbd-b80a-17067d9b9676",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_jobs = [random_tuner, bayesian_tuner]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717acace-1921-4f83-9dcb-ba83ed90c425",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deeab250-4a37-4c28-b3f9-0ce8fd1d4436",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_tuning_job(random_tuner, advanced=True, trials_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0ea0fb-9c05-4c7b-8144-77bf8be626c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Bayesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73f3e83-4f8f-4c5a-b588-0d95445b8dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_tuning_job(bayesian_tuner, advanced=True, trials_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339502fe-6f51-4f49-a662-f15eca9a2bf9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Both"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea74d389-b8de-4db9-a929-90911a9a0c1a",
   "metadata": {},
   "source": [
    "We can now view our two HPO jobs side-by-side.\n",
    "\n",
    "Observe that the trials of both jobs ran over the same time period, thus we can visually see how Bayesian search trials show improved results over time, while this is not necessarily the case for trials with Random search trials.\n",
    "\n",
    "Earlier we mentioned that we kept `k=2`, restricting our parallel jobs to 2, for both strategies. If we had increased this value hypothetically to 10 for Random search, we would see the Random search trials cluster towards the left side of the x-axis and a side-by-side temporal comparison would be less compelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19761bb4-8651-4db9-8239-fc3e25733f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_tuning_job(tuning_jobs, advanced=True, trials_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf17444-ae3a-44c0-b1c3-b613b7f0655e",
   "metadata": {},
   "source": [
    "## Warmstart HPO Job\n",
    "\n",
    "Let's now use our better understanding of good hyperparameter value ranges to do more searching. But we do not start from scratch, but [warmstart](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_HyperParameterTuningJobWarmStartConfig.html) a new HPO Job, that incorporates what was learned during the previous HPO run. Thus, we can continue optimizing our model from the point where the previous tuning experiment has finished.\n",
    "\n",
    "More information on Warm Start HPO Job Types and restrictions can be found [here](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-warm-start.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a238efc-4d6e-4985-8022-627f46078c67",
   "metadata": {},
   "source": [
    "We can compare our range definitions side-by-side to see in what direction we should point our Warmstarted HPO Job. Check the comments below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c079b444-27c4-4b0b-8e9b-fc053e5a570f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import WarmStartConfig, WarmStartTypes\n",
    "\n",
    "warm_start_config = WarmStartConfig(warm_start_type=WarmStartTypes.IDENTICAL_DATA_AND_ALGORITHM, \n",
    "                                    parents=[bayesian_tuner_name, random_tuner_name]) # Recognize how we point to the previous tuning jobs (both Bayesian and Random)\n",
    "tuner_parameters['warm_start_config'] =  warm_start_config\n",
    "\n",
    "# repeated from above, then adjusted based on the runs from above. This is just an illustration, YMMV.\n",
    "hpt_ranges = {'n-estimators': IntegerParameter(100, 300), # was (1,200). Many of the good values fell onto 200, maybe more is better?\n",
    "              'max-depth': IntegerParameter(6, 24), # was (1, 20). Many of the good values fell on 20. Maybe more is better?\n",
    "              'min-samples-leaf': IntegerParameter(1, 6), # was (1, 10) But there were not additional gains above 6.\n",
    "              'min-weight-fraction-leaf': ContinuousParameter(0.001, 0.15), # was (0.01, 0.5)\n",
    "              'max-features': CategoricalParameter(['auto', 'log2', 'sqrt'])} # unchanged. Inconclusive with the wide value ranges we searched. \n",
    "                                                                              # So let's try again with the narrower search from above.\n",
    "tuner_parameters['hyperparameter_ranges'] = hpt_ranges\n",
    "tuner_parameters['base_tuning_job_name'] = 'bayesian-warm'\n",
    "tuner_parameters['max_jobs'] = n//2 # we can reduce the number of trials, as we already build on what we learned already\n",
    "tuner_parameters['max_parallel_jobs'] = 1\n",
    "\n",
    "warmstarted_tuner = HyperparameterTuner(**tuner_parameters)\n",
    "warmstarted_tuner.fit({'train': s3_data_url}, wait=False)\n",
    "warmstarted_tuner_name = warmstarted_tuner.describe()['HyperParameterTuningJobName']\n",
    "print(f'tuning job submitted: {warmstarted_tuner_name}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992ce7b9-d5fb-4072-8738-c2cc56d486ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "warmstarted_tuner.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30f453b-68e1-4a77-a92f-2a4ce1fa4204",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_tuning_job(warmstarted_tuner, advanced=True, trials_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d781e71-c69f-46f5-8a50-6538c7d3d070",
   "metadata": {},
   "source": [
    "## Please Note\n",
    "\n",
    "As it turns out to use the existing knowledge in our heads (the understanding of hyper parameter value ranges) together with the captured knowledge in SageMaker Automatic Model Tuning (the previous trials that we now use using warmstart), we were able to continue to improve on the objective. And the overall result was much better. \n",
    "\n",
    "Also, recognize above how SageMaker AMT pre-dominantly probed the areas that were previously unexplored. The data points with the orange outline belong to the second optimization run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a44f02d-77cc-4343-9d74-8d113167fa5e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Open:\n",
    "    \n",
    "- Early Stopping?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
