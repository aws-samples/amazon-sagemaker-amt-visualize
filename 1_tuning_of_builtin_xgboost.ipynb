{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51e0cdcd",
   "metadata": {},
   "source": [
    "# Getting Started With Hyperparameter Optimization Using Amazon SageMaker Automatic Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbfc6f0",
   "metadata": {},
   "source": [
    "## Background - What are we doing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f89001",
   "metadata": {},
   "source": [
    "Machine Learning (ML) models are getting better and better. Subsequently they are taking the world by storm. Their performance relies on the right training data and choosing the right model/algorithm. But it does not end here. Typically, algorithms defer some design decisions to the ML practitioner to adopt for their specific data and task. These deferred design decisions manifest themselves as so called *hyperparameters*.\n",
    "\n",
    "What does that name mean? The result of a machine learning training, the model, can be largely seen as a collection of parameters that were learned during training. Hence the parameters that are used to configure the machine learning training are then called hyperparameters, i.e. parameters describing the creation of parameters. At any rate, they are of very practical use, such as the number of epochs to train, the learning rate, the max depth of a decision tree and so forth. And they have a major impact on the ultimate performance of your model.\n",
    "\n",
    "Selecting the right parameters and their value ranges starts with the practitioner's theoretical knowledge of the model to train. This defines the boundaries of the search space, but then it is also necessary to explore this search space empirically to find a good combination of parameters that will result in a high performing model.\n",
    "\n",
    "Luckily, we do not have to manually search that space, because SageMaker Automatic Model Tuning (AMT) is here to help. We point AMT in the right direction by defining the overall search space and AMT then takes over and coordinates the exploration of that space and fully automatically runs training jobs to efficiently evaluate the space. This then results in a good combination of hyper parameter values to train a high performing model.\n",
    "\n",
    "*In an upcoming article we will extend the notion of __just__ finding the best parameters to then __also__ include learning about the search space and to what parameter ranges a model is sensitive, as well as turning a one-shot tuning activity into a multi-step conversation with the ML practitioner, to learn together. Exciting!*\n",
    "\n",
    "In this tutorial we will walk through Amazon SageMaker Automatic Model Tuning (AMT), using a built-in XGBoost algorithm provided by Amazon SageMaker. Additional information can be found in the documentation pages below:\n",
    "* For more information on running a simple hyperparameter tuning job: https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-ex.html\n",
    "* For documentation on using the HyperParameterTuner API with the SageMaker Python SDK: https://sagemaker.readthedocs.io/en/stable/api/training/tuner.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d3c147",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda7534b",
   "metadata": {},
   "source": [
    "This notebook is split into the following sections:\n",
    "* Setup and Imports\n",
    "* Load and Prepare dataset\n",
    "* Train a SageMaker Built-In XGBoost Algorithm\n",
    "* Train and Tune a SageMaker Built-In XGBoost Algorithm\n",
    "* View the AMT job statistics \n",
    "* Visualize AMT job results and tuned Hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238d2c62",
   "metadata": {},
   "source": [
    "### Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5026ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820bc604",
   "metadata": {},
   "source": [
    "We ran this notebook using Amazon SageMaker with the version you see in the output of the next cell below. If your version is lower and you encounter issues, we recommend uncommenting the code below to upgrade your pip and SageMaker versions. Make sure to restart your kernel after upgrading for the changes to take effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae8980f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.116.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker.__version__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6e59ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade --quiet pip \"sagemaker>=2.116.0\" # upgrade SageMaker to the recommended version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e419bc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import argparse\n",
    "import traceback\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd7a5efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDK setup\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "sm = boto3.client('sagemaker')\n",
    "boto_sess = boto3.Session(region_name=region)\n",
    "sm_sess = sagemaker.session.Session(boto_session=boto_sess, sagemaker_client=sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ad8b879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-632581975302/amt-visualize-demo/data/output'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data layout and locations. \n",
    "# To store our data we are using a prefix in the Amazon SageMaker default bucket. Feel free to adjust to your preferences.\n",
    "\n",
    "BUCKET = sm_sess.default_bucket()\n",
    "PREFIX = 'amt-visualize-demo/data'\n",
    "s3_data_url = f's3://{BUCKET}/{PREFIX}'\n",
    "\n",
    "# Eventual output destination for our XGBoost model\n",
    "output_path = f's3://{BUCKET}/{PREFIX}/output'\n",
    "output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3075fb",
   "metadata": {},
   "source": [
    "## Load and Prepare dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6609672d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a9ac0a",
   "metadata": {},
   "source": [
    "The focus of this notebook is on Hyperparameter Optimization. Hence the actual task and data only play a supporting role. But to give some brief context, we are optimizing the hyperparameters of an XGBoost model that should classify handwritten digits. We use the [Optical Recognition of Handwritten Digits Data Set](https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits) via scikit-learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e1ced87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "digits         = datasets.load_digits()\n",
    "digits_df      = pd.DataFrame(digits.data)\n",
    "digits_df['y'] = digits.target\n",
    "digits_df.insert(0, 'y', digits_df.pop('y')) # XGBoost expects the target to be the first column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7511dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digit: 4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "print('digit:', int(digits_df.iloc[100].y))\n",
    "plt.imshow(digits_df.iloc[100, 1:].values.reshape(8, -1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "145da7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly sort the data then split out into train 70% and validation 30% sets\n",
    "train_data, valid_data= np.split(\n",
    "    digits_df, [int(0.7 * len(digits_df))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de42d336",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('data/train.csv', index=False, header=False)\n",
    "valid_data.to_csv('data/valid.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "448723bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "boto_sess.resource('s3').Bucket(BUCKET).Object(os.path.join(PREFIX, 'train/train.csv')).upload_file('data/train.csv')\n",
    "boto_sess.resource('s3').Bucket(BUCKET).Object(os.path.join(PREFIX, 'valid/valid.csv')).upload_file('data/valid.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a55efe",
   "metadata": {},
   "source": [
    "We upload our train and validation datasets into [Amazon S3](https://aws.amazon.com/s3/). Amazon SageMaker will launch training jobs on our behalf and those will read the data directly from S3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96665c5",
   "metadata": {},
   "source": [
    "## Train an Amazon SageMaker Built-In XGBoost Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0852f1",
   "metadata": {},
   "source": [
    "SageMaker provides many [built-in algorithms](https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html). For our example we use the popular XGBoost algorithm. To select it for our training it is identified by an image URI as described in the docs here:\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html\n",
    "\n",
    "In below code you can see how we lookup the respective image and version of the algorithm. We then define the hyperparameters and instantiate an Estimator that describes our training. For a list of hyperparameters and their meaning for this algorithm check out the [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html). \n",
    "\n",
    "Please recognize that for our small dataset we also specify the use of a small CPU based instance type for our training, specifically `ml.m5.large`.\n",
    "\n",
    "Eventually we call `fit()` to start the training and to pass in references to where our training and validation data can be found. \n",
    "\n",
    "__Side note__: Strictly speaking it would not be necessary to call the `fit()` method here for an individual training run. We will let SageMaker Automatic Model Tuning (AMT) instantiate and control the individual training runs later. We just need to pass in the Estimator we defined here as a template. We could comment out the call to `fit()`.\n",
    "\n",
    "However, calling `fit()` shows us how one training run works and helps us to get acquainted with the algorithm and the log outputs, which are also available in Amazon CloudWatch. This works even though the actual execution is happening on a different EC2 machine. SageMaker makes the log outputs directly available in this notebook here. Neat, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2db8939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm container image: 683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.5-1\n",
      "2022-11-02 20:48:21 Starting - Starting the training job...ProfilerReport-1667422101: InProgress\n",
      "...\n",
      "2022-11-02 20:49:19 Starting - Preparing the instances for training.........\n",
      "2022-11-02 20:50:40 Downloading - Downloading input data......\n",
      "2022-11-02 20:51:46 Training - Downloading the training image...\n",
      "2022-11-02 20:52:21 Training - Training image download completed. Training in progress...\u001b[34m[2022-11-02 20:52:26.129 ip-10-2-193-35.ec2.internal:1 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-11-02:20:52:26:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34m[2022-11-02:20:52:26:INFO] Failed to parse hyperparameter eval_metric value accuracy to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2022-11-02:20:52:26:INFO] Failed to parse hyperparameter objective value multi:softmax to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2022-11-02:20:52:26:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2022-11-02:20:52:26:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34m[2022-11-02:20:52:26:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2022-11-02:20:52:26:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2022-11-02:20:52:26:INFO] files path: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[2022-11-02:20:52:26:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2022-11-02:20:52:26:INFO] files path: /opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34m[2022-11-02:20:52:26:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2022-11-02:20:52:26:INFO] Single node training.\u001b[0m\n",
      "\u001b[34m[2022-11-02:20:52:26:INFO] Train matrix has 1257 rows and 64 columns\u001b[0m\n",
      "\u001b[34m[2022-11-02:20:52:26:INFO] Validation matrix has 540 rows\u001b[0m\n",
      "\u001b[34m[2022-11-02 20:52:26.333 ip-10-2-193-35.ec2.internal:1 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[20:52:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softmax' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\u001b[0m\n",
      "\u001b[34m[0]#011train-mlogloss:1.55795#011train-accuracy:0.95943#011validation-mlogloss:1.70729#011validation-accuracy:0.78889\u001b[0m\n",
      "\u001b[34m[1]#011train-mlogloss:1.20572#011train-accuracy:0.97295#011validation-mlogloss:1.42358#011validation-accuracy:0.81111\u001b[0m\n",
      "\u001b[34m[2]#011train-mlogloss:0.97027#011train-accuracy:0.97932#011validation-mlogloss:1.23074#011validation-accuracy:0.81111\u001b[0m\n",
      "\u001b[34m[3]#011train-mlogloss:0.79341#011train-accuracy:0.98568#011validation-mlogloss:1.09127#011validation-accuracy:0.82222\u001b[0m\n",
      "\u001b[34m[4]#011train-mlogloss:0.65584#011train-accuracy:0.99045#011validation-mlogloss:0.97840#011validation-accuracy:0.82963\u001b[0m\n",
      "\u001b[34m[5]#011train-mlogloss:0.54703#011train-accuracy:0.99364#011validation-mlogloss:0.89070#011validation-accuracy:0.83704\u001b[0m\n",
      "\u001b[34m[6]#011train-mlogloss:0.45843#011train-accuracy:0.99602#011validation-mlogloss:0.81601#011validation-accuracy:0.84074\u001b[0m\n",
      "\u001b[34m[7]#011train-mlogloss:0.38576#011train-accuracy:0.99602#011validation-mlogloss:0.75819#011validation-accuracy:0.84444\u001b[0m\n",
      "\u001b[34m[8]#011train-mlogloss:0.32707#011train-accuracy:0.99761#011validation-mlogloss:0.70924#011validation-accuracy:0.84815\u001b[0m\n",
      "\u001b[34m[9]#011train-mlogloss:0.27772#011train-accuracy:0.99841#011validation-mlogloss:0.66924#011validation-accuracy:0.84630\u001b[0m\n",
      "\u001b[34m[10]#011train-mlogloss:0.23722#011train-accuracy:0.99920#011validation-mlogloss:0.63535#011validation-accuracy:0.85370\u001b[0m\n",
      "\u001b[34m[11]#011train-mlogloss:0.20342#011train-accuracy:0.99920#011validation-mlogloss:0.61011#011validation-accuracy:0.85185\u001b[0m\n",
      "\u001b[34m[12]#011train-mlogloss:0.17500#011train-accuracy:0.99920#011validation-mlogloss:0.58224#011validation-accuracy:0.85000\u001b[0m\n",
      "\u001b[34m[13]#011train-mlogloss:0.15114#011train-accuracy:1.00000#011validation-mlogloss:0.56029#011validation-accuracy:0.85000\u001b[0m\n",
      "\u001b[34m[14]#011train-mlogloss:0.13116#011train-accuracy:1.00000#011validation-mlogloss:0.54051#011validation-accuracy:0.85000\u001b[0m\n",
      "\u001b[34m[15]#011train-mlogloss:0.11457#011train-accuracy:1.00000#011validation-mlogloss:0.52594#011validation-accuracy:0.84815\u001b[0m\n",
      "\n",
      "2022-11-02 20:52:53 Uploading - Uploading generated training model\n",
      "2022-11-02 20:52:53 Completed - Training job completed\n",
      "Training seconds: 138\n",
      "Billable seconds: 138\n",
      "CPU times: user 657 ms, sys: 47.1 ms, total: 704 ms\n",
      "Wall time: 4min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "\n",
    "# lookup the XGBoost image URI and build an XGBoost container\n",
    "xgboost_container = sagemaker.image_uris.retrieve('xgboost', region, '1.5-1')\n",
    "print('Algorithm container image:', xgboost_container)\n",
    "\n",
    "hyperparameters = {\n",
    "        'num_class': 10,\n",
    "        'max_depth': 5,\n",
    "        'eta':0.2,\n",
    "        'alpha': 0.2, \n",
    "        'objective':'multi:softmax',\n",
    "        'eval_metric':'accuracy',\n",
    "        'num_round':200,\n",
    "        'early_stopping_rounds': 5}\n",
    "\n",
    "# construct a SageMaker estimator that calls the XGBoost container\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri=xgboost_container, \n",
    "    hyperparameters=hyperparameters,\n",
    "    role=role,\n",
    "    instance_count=1, \n",
    "    instance_type='ml.m5.large', \n",
    "    volume_size=5, # 5 GB \n",
    "    output_path=output_path\n",
    ")\n",
    "\n",
    "# define the data type and paths to the training and validation datasets\n",
    "s3_input_train = TrainingInput(s3_data=f's3://{BUCKET}/{PREFIX}/train', content_type='csv')\n",
    "s3_input_valid = TrainingInput(s3_data=f's3://{BUCKET}/{PREFIX}/valid', content_type='csv')\n",
    "\n",
    "# execute the XGBoost training job\n",
    "estimator.fit({'train': s3_input_train, 'validation': s3_input_valid}) # Optional. See comment above this cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa0a091",
   "metadata": {},
   "source": [
    "## Train and Tune an Amazon SageMaker Built-In XGBoost Algorithm\n",
    "\n",
    "Analogous to the Estimator above we now setup the HyperparameterTuner. We pass in the estimator object as a template for the trials to be run and a range of hyperparameters to define what search space to explore. Our documentation on [tuning XGBoost](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost-tuning.html) gives you some tips on choosing parameter ranges and which objective to use.\n",
    "\n",
    "You also see that we specify the number of training jobs to be run in total, as well as how many should run in parallel. Further you can select among many [strategies](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_HyperParameterTuningJobConfig.html#sagemaker-Type-HyperParameterTuningJobConfig-Strategy). In this notebook we use `Bayesian` as strategy. It uses a systematic approach to evaluate the search space. And it incorporates knowledge gained from earlier training runs (trials) when picking better hyperparameters to evaluate in the later trials, during the same __tuning__ job run. \n",
    "\n",
    "*Bayesian Search as a strategy is an excellent default. In an upcoming notebook and article we will additionally contrast and compare different strategies.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "884541f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "n_jobs = 50\n",
    "n_parallel_jobs = 3\n",
    "\n",
    "hpt_ranges = {\n",
    "    'alpha': ContinuousParameter(0.01, .5),\n",
    "    'eta': ContinuousParameter(0.1, .5),\n",
    "    'min_child_weight': ContinuousParameter(0., 2.),\n",
    "    'max_depth': IntegerParameter(1, 10)\n",
    "}\n",
    "\n",
    "tuner_parameters = {\n",
    "    'estimator': estimator,\n",
    "    'base_tuning_job_name': 'bayesian',                   \n",
    "    'objective_metric_name': 'validation:accuracy',\n",
    "    'objective_type': 'Maximize',\n",
    "    'hyperparameter_ranges': hpt_ranges,\n",
    "    'strategy': 'Bayesian',\n",
    "    'max_jobs': n_jobs,\n",
    "    'max_parallel_jobs': n_parallel_jobs\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054c5c0f",
   "metadata": {},
   "source": [
    "Analogous to the actual training with the estimator above, we also kick things off by calling `fit()`. This time on the HyperparameterTuner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4d8fd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuning job submitted: bayesian-221102-2053.\n"
     ]
    }
   ],
   "source": [
    "tuner = HyperparameterTuner(**tuner_parameters)\n",
    "tuner.fit({'train': s3_input_train, 'validation': s3_input_valid}, wait=False)\n",
    "tuner_name = tuner.describe()['HyperParameterTuningJobName']\n",
    "print(f'tuning job submitted: {tuner_name}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cad4b4",
   "metadata": {},
   "source": [
    "Amazon SageMaker AMT, here the HyperparameterTuner, now orchestrates different training jobs (trials) over a period of time. \n",
    "\n",
    "We use `tuner.wait()` to pause this notebook's execution until the AMT job is completed and we can work with the results. Depending on the number of jobs and the level of parallelization this may take some time. For the example below it may take up to 30 minutes for 50 jobs. During this time you can view the status of your jobs in the console by navigating to Amazon SageMaker > Training > Hyperparameter tuning jobs.\n",
    "\n",
    "For more information on AMT job monitoring, see: https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-monitor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "030f8569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....................................................................................................................................................................................................................................................................................................................................................!\n"
     ]
    }
   ],
   "source": [
    "tuner.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad8e635",
   "metadata": {},
   "source": [
    "## View the AMT job statistics and results \n",
    "\n",
    "Your tuning jobs can be accessed from the Amazon SageMaker console at https://console.aws.amazon.com/sagemaker/. Select Hyperparameter tuning jobs from the Training menu to see the list. More information here: https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-monitor.html\n",
    "\n",
    "You can also check the results of the jobs programmatically and investigate the hyperparameters used, the final value achieved in the objective function and the total training time per job.\n",
    "\n",
    "Furthermore you can evaluate the results programmatically, which we will look into now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e88bbfe",
   "metadata": {},
   "source": [
    "#### 1. Via the Amazon SageMaker Python SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344a09f4",
   "metadata": {},
   "source": [
    "The SDK conveniently provides access to the tuning results as a Pandas dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b2c585",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.HyperparameterTuningJobAnalytics(tuner_name).dataframe()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f87a965",
   "metadata": {},
   "source": [
    "#### 2. Via the AWS SDK for Python (Boto3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfd2864",
   "metadata": {},
   "source": [
    "With the boto3 client we can review the results of a HPO job using [`describe_hyper_parameter_tuning_job()`](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_DescribeHyperParameterTuningJob.html) function.\n",
    "\n",
    "It returns a Python dictionary with comprehensive information. Below you see the output scoped to the `BestTrainingJob`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91af7fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sm.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=tuner_name)   # to review all data\n",
    "sm.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=tuner_name)['BestTrainingJob']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099881e5",
   "metadata": {},
   "source": [
    "We can also utilize Boto3's [`list_training_jobs_for_hyper_parameter_tuning_job()`](https://docs.aws.amazon.com/cli/latest/reference/sagemaker/list-training-jobs-for-hyper-parameter-tuning-job.html) function to review the results. This can be sorted by the value of the objective function or by the metric definitions. More functions available for Amazon SageMaker with Boto3 are described on this page: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1fac3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo_jobs = sm.list_training_jobs_for_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=tuner_name,\n",
    "    MaxResults=100,\n",
    "    SortBy='FinalObjectiveMetricValue',\n",
    "    SortOrder='Descending')\n",
    "\n",
    "for job in hpo_jobs['TrainingJobSummaries'][:10]:\n",
    "    job_descr = sm.describe_training_job(TrainingJobName=job['TrainingJobName'])\n",
    "    metrics = {m['MetricName']:  m['Value'] for m in job_descr['FinalMetricDataList']}\n",
    "    print(f'{job[\"TrainingJobName\"]} Metrics: {metrics}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b96e50",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Visualize AMT job results and tuned Hyperparameters\n",
    "\n",
    "Finally, we want to interactively visualize the impact of our hyperparameters and their values on the optimization objective.\n",
    "\n",
    "To do this, we utilize the Altair statistical visualization library for Python, and have written two custom analysis scripts `job_analytics.py` and `reporting_util.py` that we make available with this notebook.\n",
    "\n",
    "In an upcoming notebook and article we will dive deeper into the analysis of this data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdb24e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uq pip altair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df221864",
   "metadata": {},
   "source": [
    "Please ensure that the role used by SageMaker allows the `cloudwatch:ListMetrics` action on [IAM](https://console.aws.amazon.com/iam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4396d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reporting_util import analyze_hpo_job\n",
    "analyze_hpo_job(tuner, trials_only=True)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
