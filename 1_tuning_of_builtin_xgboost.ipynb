{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Amazon SageMaker Automatic Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning model training is controlled by the set of values that is refered to as hyperparameters. In contrast to parameters plugged into optimization functions, such as node weights or bias, hyperparameters are defined before model training. One can tune them manually to reach to better model performance by changing the values based on one's expertise. Alternatively, one can run hyperparameter optimization and tune these parameters automatically. \n",
    "\n",
    "Amazon SageMaker Automatic Model Tuning reduces the undifferentiated heavy lifting of researching the hyperparameter space, by launching training jobs with several sets of hyperparameter combinations and provides the set of best performing values as a result.\n",
    "\n",
    "This tutorial will walk you through SageMaker Automatic Model Tuning (AMT) using a built-in XGBoost algorithm on Amazon SageMaker. Additional information can be found on documentation pages:\n",
    "* For running a simple hyperparameter tuning job: https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-ex.html\n",
    "* For HyperParameterTuner API: https://sagemaker.readthedocs.io/en/stable/api/training/tuner.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook is split into the following sections:\n",
    "* Setup and Imports\n",
    "* Load and Prepare dataset\n",
    "* Train a SageMaker Built-In XGBoost Algorithm\n",
    "* Train and Tune a SageMaker Built-In XGBoost Algorithm\n",
    "* View the AMT job statistics \n",
    "* Visualize AMT job results and tuned Hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.112.0'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import sys\n",
    "import sagemaker\n",
    "\n",
    "#!{sys.executable} -m pip install --upgrade pip       --quiet # upgrade pip to the latest vesion\n",
    "#!{sys.executable} -m pip install --upgrade sagemaker --quiet # upgrade SageMaker to the latest vesion\n",
    "sagemaker.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import argparse\n",
    "import traceback\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDK setup\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "sm = boto3.client('sagemaker')\n",
    "boto_sess = boto3.Session(region_name=region)\n",
    "sm_sess = sagemaker.session.Session(boto_session=boto_sess, sagemaker_client=sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-632581975302/amt-visualize-demo/data/output\n"
     ]
    }
   ],
   "source": [
    "BUCKET = sm_sess.default_bucket()\n",
    "PREFIX = 'amt-visualize-demo/data'\n",
    "s3_data_url = f's3://{BUCKET}/{PREFIX}'\n",
    "\n",
    "# Eventual output destination for our XGBoost model\n",
    "output_path = f's3://{BUCKET}/{PREFIX}/output'\n",
    "print(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used in the notebook is a scikit-learn library copy of the test set of the UCI ML hand-written digits datasets https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits. Each datapoint is a 8x8 image of a digit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "digits         = datasets.load_digits()\n",
    "digits_df      = pd.DataFrame(digits.data)\n",
    "digits_df['y'] = digits.target\n",
    "digits_df.insert(0, 'y', digits_df.pop('y')) # XGBoost expects the target to be the first column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sort the data then split out into train 70% and validation 30%\n",
    "train_data, valid_data= np.split(\n",
    "    digits_df, [int(0.7 * len(digits_df))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv(\"data/train.csv\", index=False, header=False)\n",
    "valid_data.to_csv(\"data/valid.csv\", index=False, header=False) # valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We upload train and validation datasets into Amazon S3. Amazon SageMaker will interact with the data directly from S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto_sess.resource(\"s3\").Bucket(BUCKET).Object(os.path.join(PREFIX, \"train/train.csv\")\n",
    "                                                 ).upload_file(\"data/train.csv\")\n",
    "boto_sess.resource(\"s3\").Bucket(BUCKET).Object(os.path.join(PREFIX, \"valid/valid.csv\")\n",
    "                                                 ).upload_file(\"data/valid.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the built-in algorithm that comes in an image URI as described in the docs here:\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train an Amazon SageMaker Built-In XGBoost Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.5-1'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-14 13:24:38 Starting - Starting the training job...\n",
      "2022-10-14 13:25:02 Starting - Preparing the instances for trainingProfilerReport-1665753877: InProgress\n",
      ".........\n",
      "2022-10-14 13:26:29 Downloading - Downloading input data......\n",
      "2022-10-14 13:27:30 Training - Downloading the training image.....\u001b[34m[2022-10-14 13:28:15.093 ip-10-2-111-66.ec2.internal:1 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-10-14:13:28:15:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34m[2022-10-14:13:28:15:INFO] Failed to parse hyperparameter eval_metric value accuracy to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2022-10-14:13:28:15:INFO] Failed to parse hyperparameter objective value multi:softmax to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2022-10-14:13:28:15:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2022-10-14:13:28:15:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34m[2022-10-14:13:28:15:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2022-10-14:13:28:15:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2022-10-14:13:28:15:INFO] files path: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[2022-10-14:13:28:15:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2022-10-14:13:28:15:INFO] files path: /opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34m[2022-10-14:13:28:15:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2022-10-14:13:28:15:INFO] Single node training.\u001b[0m\n",
      "\u001b[34m[2022-10-14:13:28:15:INFO] Train matrix has 1257 rows and 64 columns\u001b[0m\n",
      "\u001b[34m[2022-10-14:13:28:15:INFO] Validation matrix has 540 rows\u001b[0m\n",
      "\u001b[34m[2022-10-14 13:28:15.313 ip-10-2-111-66.ec2.internal:1 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[13:28:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softmax' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\u001b[0m\n",
      "\u001b[34m[0]#011train-mlogloss:1.70928#011train-accuracy:0.86874#011validation-mlogloss:1.78571#011validation-accuracy:0.75000\u001b[0m\n",
      "\u001b[34m[1]#011train-mlogloss:1.38083#011train-accuracy:0.91249#011validation-mlogloss:1.51399#011validation-accuracy:0.76667\u001b[0m\n",
      "\u001b[34m[2]#011train-mlogloss:1.14703#011train-accuracy:0.92920#011validation-mlogloss:1.32726#011validation-accuracy:0.79074\u001b[0m\n",
      "\u001b[34m[3]#011train-mlogloss:0.97257#011train-accuracy:0.93476#011validation-mlogloss:1.17387#011validation-accuracy:0.79444\u001b[0m\n",
      "\u001b[34m[4]#011train-mlogloss:0.83478#011train-accuracy:0.94590#011validation-mlogloss:1.06231#011validation-accuracy:0.80000\u001b[0m\n",
      "\u001b[34m[5]#011train-mlogloss:0.72337#011train-accuracy:0.95943#011validation-mlogloss:0.96265#011validation-accuracy:0.82407\u001b[0m\n",
      "\u001b[34m[6]#011train-mlogloss:0.62800#011train-accuracy:0.96340#011validation-mlogloss:0.87608#011validation-accuracy:0.82963\u001b[0m\n",
      "\u001b[34m[7]#011train-mlogloss:0.55511#011train-accuracy:0.96659#011validation-mlogloss:0.81347#011validation-accuracy:0.83889\u001b[0m\n",
      "\u001b[34m[8]#011train-mlogloss:0.48675#011train-accuracy:0.97216#011validation-mlogloss:0.75469#011validation-accuracy:0.83518\u001b[0m\n",
      "\u001b[34m[9]#011train-mlogloss:0.43253#011train-accuracy:0.97613#011validation-mlogloss:0.71186#011validation-accuracy:0.84444\u001b[0m\n",
      "\u001b[34m[10]#011train-mlogloss:0.38712#011train-accuracy:0.97693#011validation-mlogloss:0.67197#011validation-accuracy:0.84630\u001b[0m\n",
      "\u001b[34m[11]#011train-mlogloss:0.34636#011train-accuracy:0.97852#011validation-mlogloss:0.63897#011validation-accuracy:0.85741\u001b[0m\n",
      "\u001b[34m[12]#011train-mlogloss:0.31253#011train-accuracy:0.98011#011validation-mlogloss:0.60873#011validation-accuracy:0.85556\u001b[0m\n",
      "\u001b[34m[13]#011train-mlogloss:0.28397#011train-accuracy:0.98329#011validation-mlogloss:0.58436#011validation-accuracy:0.85556\u001b[0m\n",
      "\u001b[34m[14]#011train-mlogloss:0.25879#011train-accuracy:0.98329#011validation-mlogloss:0.56093#011validation-accuracy:0.85556\u001b[0m\n",
      "\u001b[34m[15]#011train-mlogloss:0.23662#011train-accuracy:0.98489#011validation-mlogloss:0.54025#011validation-accuracy:0.86482\u001b[0m\n",
      "\u001b[34m[16]#011train-mlogloss:0.21617#011train-accuracy:0.98489#011validation-mlogloss:0.52479#011validation-accuracy:0.86482\u001b[0m\n",
      "\u001b[34m[17]#011train-mlogloss:0.19816#011train-accuracy:0.98648#011validation-mlogloss:0.50475#011validation-accuracy:0.87037\u001b[0m\n",
      "\u001b[34m[18]#011train-mlogloss:0.18460#011train-accuracy:0.98886#011validation-mlogloss:0.49436#011validation-accuracy:0.86482\u001b[0m\n",
      "\u001b[34m[19]#011train-mlogloss:0.17119#011train-accuracy:0.98807#011validation-mlogloss:0.48170#011validation-accuracy:0.86852\u001b[0m\n",
      "\u001b[34m[20]#011train-mlogloss:0.15983#011train-accuracy:0.99045#011validation-mlogloss:0.46881#011validation-accuracy:0.86852\u001b[0m\n",
      "\u001b[34m[21]#011train-mlogloss:0.14990#011train-accuracy:0.99204#011validation-mlogloss:0.46331#011validation-accuracy:0.87037\u001b[0m\n",
      "\u001b[34m[22]#011train-mlogloss:0.14094#011train-accuracy:0.99045#011validation-mlogloss:0.45719#011validation-accuracy:0.87222\u001b[0m\n",
      "\u001b[34m[23]#011train-mlogloss:0.13382#011train-accuracy:0.99045#011validation-mlogloss:0.44846#011validation-accuracy:0.87037\u001b[0m\n",
      "\u001b[34m[24]#011train-mlogloss:0.12719#011train-accuracy:0.99125#011validation-mlogloss:0.44307#011validation-accuracy:0.87037\u001b[0m\n",
      "\u001b[34m[25]#011train-mlogloss:0.12034#011train-accuracy:0.99204#011validation-mlogloss:0.43429#011validation-accuracy:0.87037\u001b[0m\n",
      "\u001b[34m[26]#011train-mlogloss:0.11518#011train-accuracy:0.99284#011validation-mlogloss:0.43131#011validation-accuracy:0.87222\u001b[0m\n",
      "\u001b[34m[27]#011train-mlogloss:0.11012#011train-accuracy:0.99364#011validation-mlogloss:0.42623#011validation-accuracy:0.87407\u001b[0m\n",
      "\u001b[34m[28]#011train-mlogloss:0.10576#011train-accuracy:0.99364#011validation-mlogloss:0.42050#011validation-accuracy:0.87222\u001b[0m\n",
      "\u001b[34m[29]#011train-mlogloss:0.10172#011train-accuracy:0.99284#011validation-mlogloss:0.42063#011validation-accuracy:0.87037\u001b[0m\n",
      "\u001b[34m[30]#011train-mlogloss:0.09732#011train-accuracy:0.99284#011validation-mlogloss:0.41628#011validation-accuracy:0.87222\u001b[0m\n",
      "\u001b[34m[31]#011train-mlogloss:0.09364#011train-accuracy:0.99602#011validation-mlogloss:0.41215#011validation-accuracy:0.87407\u001b[0m\n",
      "\u001b[34m[32]#011train-mlogloss:0.09089#011train-accuracy:0.99761#011validation-mlogloss:0.41160#011validation-accuracy:0.87407\u001b[0m\n",
      "\u001b[34m[33]#011train-mlogloss:0.08868#011train-accuracy:0.99761#011validation-mlogloss:0.41248#011validation-accuracy:0.87037\u001b[0m\n",
      "\u001b[34m[34]#011train-mlogloss:0.08620#011train-accuracy:0.99761#011validation-mlogloss:0.41107#011validation-accuracy:0.87222\u001b[0m\n",
      "\u001b[34m[35]#011train-mlogloss:0.08455#011train-accuracy:0.99761#011validation-mlogloss:0.41037#011validation-accuracy:0.87222\u001b[0m\n",
      "\u001b[34m[36]#011train-mlogloss:0.08296#011train-accuracy:0.99841#011validation-mlogloss:0.41063#011validation-accuracy:0.87222\u001b[0m\n",
      "\u001b[34m[37]#011train-mlogloss:0.08112#011train-accuracy:0.99841#011validation-mlogloss:0.40648#011validation-accuracy:0.87222\u001b[0m\n",
      "\u001b[34m[38]#011train-mlogloss:0.07997#011train-accuracy:0.99841#011validation-mlogloss:0.40715#011validation-accuracy:0.87037\u001b[0m\n",
      "\u001b[34m[39]#011train-mlogloss:0.07868#011train-accuracy:0.99841#011validation-mlogloss:0.40550#011validation-accuracy:0.87037\u001b[0m\n",
      "\u001b[34m[40]#011train-mlogloss:0.07812#011train-accuracy:0.99841#011validation-mlogloss:0.40479#011validation-accuracy:0.86852\u001b[0m\n",
      "\u001b[34m[41]#011train-mlogloss:0.07676#011train-accuracy:0.99841#011validation-mlogloss:0.40063#011validation-accuracy:0.87222\u001b[0m\n",
      "\u001b[34m[42]#011train-mlogloss:0.07644#011train-accuracy:0.99920#011validation-mlogloss:0.39954#011validation-accuracy:0.87222\u001b[0m\n",
      "\u001b[34m[43]#011train-mlogloss:0.07553#011train-accuracy:0.99920#011validation-mlogloss:0.39913#011validation-accuracy:0.87593\u001b[0m\n",
      "\u001b[34m[44]#011train-mlogloss:0.07482#011train-accuracy:0.99841#011validation-mlogloss:0.39815#011validation-accuracy:0.87407\u001b[0m\n",
      "\u001b[34m[45]#011train-mlogloss:0.07439#011train-accuracy:0.99920#011validation-mlogloss:0.39840#011validation-accuracy:0.87222\u001b[0m\n",
      "\u001b[34m[46]#011train-mlogloss:0.07310#011train-accuracy:0.99920#011validation-mlogloss:0.39876#011validation-accuracy:0.87407\u001b[0m\n",
      "\u001b[34m[47]#011train-mlogloss:0.07295#011train-accuracy:0.99920#011validation-mlogloss:0.39823#011validation-accuracy:0.87222\u001b[0m\n",
      "\u001b[34m[48]#011train-mlogloss:0.07266#011train-accuracy:0.99920#011validation-mlogloss:0.39838#011validation-accuracy:0.87222\u001b[0m\n",
      "\u001b[34m[49]#011train-mlogloss:0.07234#011train-accuracy:0.99920#011validation-mlogloss:0.39872#011validation-accuracy:0.87037\u001b[0m\n",
      "\n",
      "2022-10-14 13:28:31 Uploading - Uploading generated training model\n",
      "2022-10-14 13:29:03 Completed - Training job completed\n",
      "Training seconds: 137\n",
      "Billable seconds: 137\n",
      "CPU times: user 676 ms, sys: 41.6 ms, total: 717 ms\n",
      "Wall time: 4min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "hyperparameters = {\n",
    "        \"num_class\": \"10\",\n",
    "        \"max_depth\":\"5\",\n",
    "        \"eta\":\"0.2\",\n",
    "        \"gamma\":\"1\",\n",
    "        \"min_child_weight\":\"6\",\n",
    "        \"subsample\":\"0.7\",\n",
    "        \"objective\":\"multi:softmax\",\n",
    "        \"eval_metric\":\"accuracy\",\n",
    "        \"num_round\":\"50\"}\n",
    "\n",
    "# lookup the XGBoost image URI and builds an XGBoost container\n",
    "xgboost_container = sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.5-1\")\n",
    "display(xgboost_container)\n",
    "\n",
    "# construct a SageMaker estimator that calls the XGBoost container\n",
    "estimator = sagemaker.estimator.Estimator(image_uri=xgboost_container, \n",
    "                                          hyperparameters=hyperparameters,\n",
    "                                          role=role,\n",
    "                                          instance_count=1, \n",
    "                                          instance_type='ml.m5.large', \n",
    "                                          volume_size=5, # 5 GB \n",
    "                                          output_path=output_path)\n",
    "\n",
    "# define the data type and paths to the training and validation datasets\n",
    "s3_input_train = TrainingInput(\n",
    "    s3_data=f's3://{BUCKET}/{PREFIX}/train', content_type=\"csv\")\n",
    "s3_input_valid = TrainingInput(\n",
    "    s3_data=f's3://{BUCKET}/{PREFIX}/valid', content_type=\"csv\")\n",
    "\n",
    "# execute the XGBoost training job\n",
    "estimator.fit({'train': s3_input_train, 'validation': s3_input_valid})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Tune an Amazon SageMaker Built-In XGBoost Algorithm\n",
    "\n",
    "Amazon SageMaker AMT now orchestrates different trials. We use `tuner.wait()` to pause notebook execution until the AMT job is completed. Depending on the number of jobs and teh configuration of their paralelization this may take a while. For the example above it may take around 10 minutes. During this time you can view the status of your jobs in the console by navigating to Amazon SageMaker > Training > Hyperparameter tuning jobs.\n",
    "\n",
    "For more information on AMT job monitoring, see: https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-monitor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter\n",
    "from sagemaker.tuner import ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "n_jobs = 50\n",
    "n_parallel_jobs = 2\n",
    "\n",
    "# redundant declaration - included for visibility \n",
    "hyperparameters = {\n",
    "        \"num_class\": \"10\",\n",
    "        \"max_depth\":\"5\",\n",
    "        \"eta\":\"0.2\",\n",
    "        \"gamma\":\"1\",\n",
    "        \"min_child_weight\":\"6\",\n",
    "        \"subsample\":\"0.7\",\n",
    "        \"objective\":\"multi:softmax\",\n",
    "        \"eval_metric\":\"accuracy\",\n",
    "        \"num_round\":\"50\"}\n",
    "\n",
    "hpt_ranges = {'eta': IntegerParameter(0, 1),\n",
    "              'alpha': IntegerParameter(0, 2),\n",
    "              'min_child_weight': IntegerParameter(1, 10),\n",
    "              'max_depth': IntegerParameter(1, 20)\n",
    "             }\n",
    "\n",
    "tuner_parameters = {'estimator': estimator,\n",
    "                    'base_tuning_job_name': 'bayesian',                   \n",
    "                    'objective_metric_name': 'validation:accuracy',\n",
    "                    'objective_type': 'Maximize',\n",
    "                    'hyperparameter_ranges': hpt_ranges,\n",
    "                    'strategy': 'Bayesian',\n",
    "                    'max_jobs': n_jobs,\n",
    "                    'max_parallel_jobs': n_parallel_jobs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuning job submitted: bayesian-221014-1329.\n"
     ]
    }
   ],
   "source": [
    "tuner = HyperparameterTuner(**tuner_parameters)\n",
    "tuner.fit({'train': s3_input_train, 'validation': s3_input_valid}, wait=False)\n",
    "tuner_name = tuner.describe()[\"HyperParameterTuningJobName\"]\n",
    "print(f'tuning job submitted: {tuner_name}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............................................................................................................................................................................................................."
     ]
    }
   ],
   "source": [
    "tuner.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the AMT job statistics and results \n",
    "\n",
    "Tuning jobs you have run can be accessed from the Amazon SageMaker console at https://console.aws.amazon.com/sagemaker/. Select Hyperparameter tuning job from the Training menu to see the list. More information here: https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-monitor.html\n",
    "\n",
    "We can check the results of the HPO jobs and investigate the hyperparameters used, the final value achieved in the objective function and the total training time per job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This can be done either via the Amazon SageMaker Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.HyperparameterTuningJobAnalytics(tuner_name).dataframe()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Or via the AWS SDK for Python (Boto3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the boto3 client we review the results of HPO job using describe_hyper_parameter_tuning_job() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sm.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=tuner_name)   # to review all statistics\n",
    "sm.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=tuner_name)['BestTrainingJob']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also utilize the boto3 list_training_jobs_for_hyper_parameter_tuning_job() function to review the results sorted by the value of the objective function and including metric definitions. More functions available for Amazon SageMaker with boto3 are described on this page: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo_jobs = sm.list_training_jobs_for_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=tuner_name,\n",
    "    MaxResults=100,\n",
    "    SortBy='FinalObjectiveMetricValue',\n",
    "    SortOrder='Descending')\n",
    "\n",
    "for job in hpo_jobs['TrainingJobSummaries']:\n",
    "    job_descr = sm.describe_training_job(TrainingJobName=job['TrainingJobName'])\n",
    "    metrics = {m['MetricName']:  m['Value'] for m in job_descr['FinalMetricDataList']}\n",
    "    print(f'{(job['TrainingJobName']} Metrics: {metrics}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize AMT job results and tuned Hyperparameters\n",
    "\n",
    "Finally, we want to visualise the behaviour of our hyperparameters at different values.\n",
    "\n",
    "To do this, we are using the altair library, and have written two custom analysis scripts `job_analytics.py` and `reporting_util.py` that we make available with this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uq altair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from job_analytics import *\n",
    "from reporting_util import *\n",
    "\n",
    "import altair as alt\n",
    "\n",
    "\n",
    "_ = alt.data_transformers.disable_max_rows()\n",
    "alt.renderers.enable('mimetype')\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', None)  # Don't truncate TrainingJobName        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please ensure that the role used by SageMaker allows the cloudwatch:ListMetrics action on IAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_hpo_job(tuner)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
