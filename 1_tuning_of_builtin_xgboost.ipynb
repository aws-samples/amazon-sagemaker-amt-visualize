{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Amazon SageMaker Automatic Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning model training is controlled by the set of values that is refered to as hyperparameters. In contrast to parameters plugged into optimization functions, such as node weights or bias, hyperparameters are defined before model training. With experience and expertise, one can tune them manually to obtain better model performance. Alternatively, this task can be automated by performing hyperparameter optimization to tune a range of values automatically. In this notebook we demonstrate how to achieve this by leveraging the Amazon SageMaker Automatic Model Tuning feature.\n",
    "\n",
    "Amazon SageMaker Automatic Model Tuning (AMT) reduces the undifferentiated heavy lifting of researching the hyperparameter space, by launching training jobs with several sets of hyperparameter combinations and provides the set of best performing values as a result.\n",
    "\n",
    "This tutorial will walk you through the Amazon SageMaker Automatic Model Tuning (AMT) feature, using a built-in XGBoost algorithm provided by the Amazon SageMaker library. Additional information can be found in the documentation pages below:\n",
    "* For more information on running a simple hyperparameter tuning job: https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-ex.html\n",
    "* For documentation on using the HyperParameterTuner API with the SageMaker Python SDK: https://sagemaker.readthedocs.io/en/stable/api/training/tuner.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is split into the following sections:\n",
    "* Setup and Imports\n",
    "* Load and Prepare dataset\n",
    "* Train a SageMaker Built-In XGBoost Algorithm\n",
    "* Train and Tune a SageMaker Built-In XGBoost Algorithm\n",
    "* View the AMT job statistics \n",
    "* Visualize AMT job results and tuned Hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, we recommend using Amazon SageMaker version 2.88.1 or higher, if your version is lower and you encounter issues, we recommend uncommenting the code below to upgrade your pip and sagemaker versions. Make sure to restart your kernel after upgrading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker.__version__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "#!{sys.executable} -m pip install --upgrade pip       --quiet # upgrade pip to the latest vesion\n",
    "#!{sys.executable} -m pip install --upgrade sagemaker==2.114.0 --quiet # upgrade SageMaker to the recommended version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import argparse\n",
    "import traceback\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDK setup\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "sm = boto3.client('sagemaker')\n",
    "boto_sess = boto3.Session(region_name=region)\n",
    "sm_sess = sagemaker.session.Session(boto_session=boto_sess, sagemaker_client=sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = sm_sess.default_bucket()\n",
    "PREFIX = 'amt-visualize-demo/data'\n",
    "s3_data_url = f's3://{BUCKET}/{PREFIX}'\n",
    "\n",
    "# eventual output destination for our XGBoost model\n",
    "output_path = f's3://{BUCKET}/{PREFIX}/output'\n",
    "output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used in the notebook is a scikit-learn library copy of the test set of the UCI ML hand-written digits datasets https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits. Each datapoint is a 8x8 image of a digit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "digits         = datasets.load_digits()\n",
    "digits_df      = pd.DataFrame(digits.data)\n",
    "digits_df['y'] = digits.target\n",
    "digits_df.insert(0, 'y', digits_df.pop('y')) # XGBoost expects the target to be the first column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly sort the data then split out into train 70% and validation 30% sets\n",
    "train_data, valid_data= np.split(\n",
    "    digits_df, [int(0.7 * len(digits_df))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv(\"data/train.csv\", index=False, header=False)\n",
    "valid_data.to_csv(\"data/valid.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We upload train and validation datasets into [Amazon S3](https://aws.amazon.com/s3/). Amazon SageMaker will interact with the data directly from S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto_sess.resource(\"s3\").Bucket(BUCKET).Object(os.path.join(PREFIX, \"train/train.csv\")\n",
    "                                                 ).upload_file(\"data/train.csv\")\n",
    "boto_sess.resource(\"s3\").Bucket(BUCKET).Object(os.path.join(PREFIX, \"valid/valid.csv\")\n",
    "                                                 ).upload_file(\"data/valid.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the built-in algorithm that comes in an image URI as described in the docs here:\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train an Amazon SageMaker Built-In XGBoost Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "hyperparameters = {\n",
    "        \"num_class\": \"10\",\n",
    "        \"max_depth\":\"5\",\n",
    "        \"eta\":\"0.2\",\n",
    "        \"gamma\":\"1\",\n",
    "        \"min_child_weight\":\"6\",\n",
    "        \"subsample\":\"0.7\",\n",
    "        \"objective\":\"multi:softmax\",\n",
    "        \"eval_metric\":\"accuracy\",\n",
    "        \"num_round\":\"50\"}\n",
    "\n",
    "# lookup the XGBoost image URI and builds an XGBoost container\n",
    "xgboost_container = sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.5-1\")\n",
    "print(xgboost_container)\n",
    "\n",
    "# construct a SageMaker estimator that calls the XGBoost container\n",
    "estimator = sagemaker.estimator.Estimator(image_uri=xgboost_container, \n",
    "                                          hyperparameters=hyperparameters,\n",
    "                                          role=role,\n",
    "                                          instance_count=1, \n",
    "                                          instance_type='ml.m5.large', \n",
    "                                          volume_size=5, # 5 GB \n",
    "                                          output_path=output_path)\n",
    "\n",
    "# define the data type and paths to the training and validation datasets\n",
    "s3_input_train = TrainingInput(\n",
    "    s3_data=f's3://{BUCKET}/{PREFIX}/train', content_type=\"csv\")\n",
    "s3_input_valid = TrainingInput(\n",
    "    s3_data=f's3://{BUCKET}/{PREFIX}/valid', content_type=\"csv\")\n",
    "\n",
    "# execute the XGBoost training job\n",
    "estimator.fit({'train': s3_input_train, 'validation': s3_input_valid})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Tune an Amazon SageMaker Built-In XGBoost Algorithm\n",
    "\n",
    "Amazon SageMaker AMT now orchestrates different trials. We use `tuner.wait()` to pause notebook execution until the AMT job is completed. Depending on the number of jobs and the level parallelization this may take some time. For the example below it may take up to 30 minutes for 50 jobs. During this time you can view the status of your jobs in the console by navigating to Amazon SageMaker > Training > Hyperparameter tuning jobs.\n",
    "\n",
    "For more information on AMT job monitoring, see: https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-monitor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter\n",
    "from sagemaker.tuner import ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "n_jobs = 50\n",
    "n_parallel_jobs = 2\n",
    "\n",
    "# redundant declaration - included for visibility \n",
    "hyperparameters = {\n",
    "        \"num_class\": \"10\",\n",
    "        \"max_depth\":\"5\",\n",
    "        \"eta\":\"0.2\",\n",
    "        \"gamma\":\"1\",\n",
    "        \"min_child_weight\":\"6\",\n",
    "        \"subsample\":\"0.7\",\n",
    "        \"objective\":\"multi:softmax\",\n",
    "        \"eval_metric\":\"accuracy\",\n",
    "        \"num_round\":\"50\"}\n",
    "\n",
    "hpt_ranges = {'eta': IntegerParameter(0, 1),\n",
    "              'alpha': IntegerParameter(0, 2),\n",
    "              'min_child_weight': IntegerParameter(1, 10),\n",
    "              'max_depth': IntegerParameter(1, 20)\n",
    "             }\n",
    "\n",
    "tuner_parameters = {'estimator': estimator,\n",
    "                    'base_tuning_job_name': 'bayesian',                   \n",
    "                    'objective_metric_name': 'validation:accuracy',\n",
    "                    'objective_type': 'Maximize',\n",
    "                    'hyperparameter_ranges': hpt_ranges,\n",
    "                    'strategy': 'Bayesian',\n",
    "                    'max_jobs': n_jobs,\n",
    "                    'max_parallel_jobs': n_parallel_jobs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = HyperparameterTuner(**tuner_parameters)\n",
    "tuner.fit({'train': s3_input_train, 'validation': s3_input_valid}, wait=False)\n",
    "tuner_name = tuner.describe()[\"HyperParameterTuningJobName\"]\n",
    "print(f'tuning job submitted: {tuner_name}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the AMT job statistics and results \n",
    "\n",
    "Your tuning jobs can be accessed from the Amazon SageMaker console at https://console.aws.amazon.com/sagemaker/. Select Hyperparameter tuning jobs from the Training menu to see the list. More information here: https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-monitor.html\n",
    "\n",
    "You can also check the results of the jobs programmatically and investigate the hyperparameters used, the final value achieved in the objective function and the total training time per job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Via the Amazon SageMaker Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.HyperparameterTuningJobAnalytics(tuner_name).dataframe()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Via the AWS SDK for Python (Boto3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the boto3 client we review the results of HPO job using [`describe_hyper_parameter_tuning_job()`](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_DescribeHyperParameterTuningJob.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sm.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=tuner_name)   # to review all statistics\n",
    "sm.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=tuner_name)['BestTrainingJob']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also utilize the Boto3 [`list_training_jobs_for_hyper_parameter_tuning_job()`](https://docs.aws.amazon.com/cli/latest/reference/sagemaker/list-training-jobs-for-hyper-parameter-tuning-job.html) function to review the results. This can be sorted by the value of the objective function or by the metric definitions. More functions available for Amazon SageMaker with Boto3 are described on this page: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo_jobs = sm.list_training_jobs_for_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=tuner_name,\n",
    "    MaxResults=100,\n",
    "    SortBy='FinalObjectiveMetricValue',\n",
    "    SortOrder='Descending')\n",
    "\n",
    "for job in hpo_jobs['TrainingJobSummaries'][:10]:\n",
    "    job_descr = sm.describe_training_job(TrainingJobName=job['TrainingJobName'])\n",
    "    metrics = {m['MetricName']:  m['Value'] for m in job_descr['FinalMetricDataList']}\n",
    "    print(f'{job[\"TrainingJobName\"]} Metrics: {metrics}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Visualize AMT job results and tuned Hyperparameters\n",
    "\n",
    "Finally, we want to visualise the behaviour of our hyperparameters at different values.\n",
    "\n",
    "To do this, we are utilize the Vega-Altair statistical visualization library for Python, and have written two custom analysis scripts `job_analytics.py` and `reporting_util.py` that we make available with this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uq pip altair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please ensure that the role used by SageMaker allows the `cloudwatch:ListMetrics` action on [IAM](https://console.aws.amazon.com/iam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reporting_util import analyze_hpo_job\n",
    "analyze_hpo_job(tuner)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
